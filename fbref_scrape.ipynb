{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87e3e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in ./opt/anaconda3/lib/python3.9/site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1023f721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'searia_blog'\n",
      "/Users/damatta/searia_blog\n"
     ]
    }
   ],
   "source": [
    "%cd searia_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8d2010bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Squad</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flamengo</td>\n",
       "      <td>639950ae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Squad        id\n",
       "0  Flamengo  639950ae"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Squad': ['Flamengo'],#, 'Palmeiras','Botafogo'], \n",
    "    'id': ['639950ae']}#, 'abdce579','d9fdd9d9']}\n",
    "\n",
    "# data = {\n",
    "#     'Squad': ['Botafogo'],#, 'Palmeiras','Botafogo'], \n",
    "#     'id': ['d9fdd9d9']}#, 'abdce579','d9fdd9d9']}\n",
    "\n",
    "teams = pd.DataFrame(data)\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2b352824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://fbref.com/en/squads/639950ae/Flamengo-Stats\n",
    "tables = ['matchlogs_for']\n",
    "years = ['2024']\n",
    "\n",
    "def fetch_team_stats(table, id, year):\n",
    "    url = f\"https://fbref.com/en/squads/{id}/{year}\"\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    html = soup.find(\"table\", id=table)\n",
    "    #read into a df\n",
    "    df = pd.read_html(str(html))[0]\n",
    "    \n",
    "    match_ids = []\n",
    "    for row in html.tbody.find_all(\"tr\"):\n",
    "        link = row.find(\"a\", string=\"Match Report\")\n",
    "        if link:\n",
    "            m = re.search(r'/en/matches/([0-9a-f]+)/', link[\"href\"])\n",
    "            match_ids.append(m.group(1))\n",
    "        else:\n",
    "            match_ids.append(None)  # or '' if you prefer\n",
    "            \n",
    "    df[\"match_id\"] = match_ids\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "for id in teams['id']:\n",
    "    team = teams.loc[teams['id'] == id, 'Squad'].iloc[0]\n",
    "    print(id, team)\n",
    "    for table in tables:\n",
    "        for year in years:\n",
    "            try:\n",
    "                df = fetch_team_stats(table, id, year)\n",
    "    #             print(df.head())\n",
    "#                 print(f\"{table}_{team}_{year}.csv\")\n",
    "                df.to_csv(f\"{table}_{team}_{year}.csv\", index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"No table found for: {table} in {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f00868ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url(\"https://fbref.com/robots.txt\")\n",
    "rp.read()\n",
    "\n",
    "url = \"https://fbref.com/en/matches/a49ba761/\"\n",
    "if not rp.can_fetch(\"*\", url):\n",
    "    raise RuntimeError(\"URL disallowed by robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bf57879e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3932504637.py, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[188], line 45\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"TimedOut: {table} in {match_id}\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "squads = teams['Squad'].to_numpy()\n",
    "\n",
    "tables = ['stats_639950ae_summary', 'stats_639950ae_passing', 'stats_639950ae_passing_types', 'stats_639950ae_defense',\n",
    "         'stats_639950ae_possession', 'stats_639950ae_misc']\n",
    "\n",
    "def fetch_match_stats(table, id):\n",
    "    url = f\"https://fbref.com/en/matches/{id}/\"\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    html = soup.find(\"table\", id=table)\n",
    "    #read into a df\n",
    "    df = pd.read_html(str(html))[0]\n",
    "                \n",
    "    df[\"match_id\"] = id\n",
    "    \n",
    "    df.columns = [col[1] if col[0] == \"Unnamed: 0\" else col[1] or col[0] for col in df.columns.values]\n",
    "\n",
    "    return df\n",
    "\n",
    "for squad in squads:\n",
    "    \n",
    "    match_ids = pd.read_csv(f\"matchlogs_for_{squad}_{year}.csv\")\n",
    "\n",
    "    match_ids = match_ids[match_ids['match_id'].notna()]\n",
    "    match_ids = match_ids.head(2)\n",
    "#     match_ids = df.loc[3:4]\n",
    "#     match_ids = df.loc[5:6]\n",
    "#     match_ids = df.loc[7:8]\n",
    "#     match_ids = df.loc[9:10]\n",
    "#     match_ids = df.loc[11:12]\n",
    "\n",
    "    match_ids = match_ids['match_id'].to_numpy()\n",
    "#     print(match_ids)\n",
    "    \n",
    "    for match_id in match_ids:\n",
    "        print(match_id)\n",
    "        for table in tables:\n",
    "            try:\n",
    "                df = fetch_match_stats(table, match_id)\n",
    "                df.to_csv(f\"{table}_{match_id}.csv\", index=False)\n",
    "#                   print(f\"{table}_{match_id}.csv\")\n",
    "            except Exception as e:\n",
    "                print(f\"TimedOut: {table} in {match_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a0477c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing match 471380e2\n",
      "  Saved stats_639950ae_summary_471380e2.csv\n",
      "  Saved stats_639950ae_passing_471380e2.csv\n",
      "  Saved stats_639950ae_passing_types_471380e2.csv\n",
      "  Saved stats_639950ae_defense_471380e2.csv\n",
      "  Saved stats_639950ae_possession_471380e2.csv\n",
      "  Saved stats_639950ae_misc_471380e2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "year = 2024\n",
    "\n",
    "# -------------------------------\n",
    "# PoliteFetcher class definition\n",
    "# -------------------------------\n",
    "class PoliteFetcher:\n",
    "    def __init__(self, base_url, user_agent, default_delay=8, cache_dir=None):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.user_agent = user_agent\n",
    "        self.default_delay = default_delay\n",
    "        self.cache_dir = cache_dir\n",
    "        self._init_robot_parser()\n",
    "\n",
    "        if cache_dir:\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    def _init_robot_parser(self):\n",
    "        robots_url = f\"{self.base_url}/robots.txt\"\n",
    "        self.rp = RobotFileParser()\n",
    "        self.rp.set_url(robots_url)\n",
    "        self.rp.read()\n",
    "        # Use crawl-delay if specified, else fallback\n",
    "        self.delay = self.rp.crawl_delay('*') or self.default_delay\n",
    "\n",
    "    def fetch(self, path, max_retries=10):\n",
    "        \"\"\"\n",
    "        Fetches `base_url + path` if allowed. Honors crawl-delay,\n",
    "        optionally caches pages, and retries on 429 with exponential backoff.\n",
    "        \"\"\"\n",
    "        full_url = f\"{self.base_url}{path}\"\n",
    "        # Check robots.txt permission\n",
    "        if not self.rp.can_fetch('*', full_url):\n",
    "            raise PermissionError(f\"Blocked by robots.txt: {full_url}\")\n",
    "\n",
    "        # Prepare cache filename if caching enabled\n",
    "        cache_path = None\n",
    "        if self.cache_dir:\n",
    "            safe_name = path.strip('/').replace('/', '_')\n",
    "            cache_path = os.path.join(self.cache_dir, f\"{safe_name}.html\")\n",
    "            if os.path.exists(cache_path):\n",
    "                with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                    return BeautifulSoup(f.read(), 'html.parser')\n",
    "\n",
    "        headers = {'User-Agent': self.user_agent}\n",
    "        wait = self.delay\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            resp = requests.get(full_url, headers=headers)\n",
    "            if resp.status_code == 200:\n",
    "                # Cache the page if required\n",
    "                if cache_path:\n",
    "                    with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(resp.text)\n",
    "                time.sleep(self.delay)\n",
    "                return BeautifulSoup(resp.text, 'html.parser')\n",
    "            elif resp.status_code == 429:\n",
    "                print(f\"[429] Rate limited. Backing off for {wait}s (attempt {attempt})\")\n",
    "                time.sleep(wait)\n",
    "                wait *= 2\n",
    "            else:\n",
    "                resp.raise_for_status()\n",
    "\n",
    "        raise RuntimeError(f\"Failed to fetch {full_url} after {max_retries} retries\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Initialization and configuration block\n",
    "# ---------------------------------------\n",
    "# Assumes 'teams' (DataFrame with 'Squad') and 'year' are defined\n",
    "# Example:\n",
    "# teams = pd.read_csv(\"teams.csv\")\n",
    "# year = 2025\n",
    "\n",
    "fetcher = PoliteFetcher(\n",
    "    base_url=\"https://fbref.com\",\n",
    "    user_agent=\"MyAnalyticsBot/1.0 (+https://yourdomain.com/contact)\",\n",
    "    default_delay=8,\n",
    "    cache_dir=\"fbref_cache\"\n",
    ")\n",
    "\n",
    "tables = [\n",
    "    'stats_639950ae_summary', 'stats_639950ae_passing', 'stats_639950ae_passing_types',\n",
    "    'stats_639950ae_defense', 'stats_639950ae_possession', 'stats_639950ae_misc'\n",
    "]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Main scraping loop\n",
    "# ---------------------------------------\n",
    "squads = teams['Squad'].to_numpy()\n",
    "\n",
    "for squad in squads:\n",
    "    # Load match IDs for this squad\n",
    "    match_logs_file = f\"matchlogs_for_{squad}_{year}.csv\"\n",
    "    match_ids_df = pd.read_csv(match_logs_file)\n",
    "    match_ids = match_ids_df['match_id'].dropna().astype(str).tolist()\n",
    "    match_ids = ['471380e2']\n",
    "\n",
    "    for match_id in match_ids:\n",
    "        print(f\"Processing match {match_id}\")\n",
    "        for table in tables:\n",
    "            try:\n",
    "                # Fetch and parse the page\n",
    "                soup = fetcher.fetch(f\"/en/matches/{match_id}/\")\n",
    "                html_table = soup.find(\"table\", id=table)\n",
    "                if html_table is None:\n",
    "                    print(f\"  Table {table} not found for match {match_id}\")\n",
    "                    continue\n",
    "\n",
    "                # Read table into DataFrame\n",
    "                df = pd.read_html(str(html_table))[0]\n",
    "                # Add match_id column\n",
    "                df[\"match_id\"] = match_id\n",
    "                df['match_id'] = df['match_id'].astype(str)\n",
    "                # Clean column names\n",
    "                df.columns = [\n",
    "                    col[1] if col[0] == \"Unnamed: 0\" else col[1] or col[0]\n",
    "                    for col in df.columns.values\n",
    "                ]\n",
    "                # Save to CSV\n",
    "                out_file = f\"{table}_{match_id}.csv\"\n",
    "                df.to_csv(out_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "                print(f\"  Saved {out_file}\")\n",
    "            except PermissionError as e:\n",
    "                print(f\"  PermissionError: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error fetching {table} for match {match_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f394b74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['44e47b4d',\n",
       " '066baefb',\n",
       " '8aacc798',\n",
       " '6d04e942',\n",
       " '471380e2',\n",
       " '1c1074ee',\n",
       " 'b4070bcd',\n",
       " '53554f9f',\n",
       " 'ead21e28',\n",
       " 'c8fe4906',\n",
       " '4db5fbff',\n",
       " 'a49ba761']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f6c92ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639950ae Flamengo\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34378c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3e568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8c1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e8bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dc394364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• (no id)\n",
      "• (no id)\n",
      "• (no id)\n",
      "• stats_639950ae_summary\n",
      "• stats_639950ae_passing\n",
      "• stats_639950ae_passing_types\n",
      "• stats_639950ae_defense\n",
      "• stats_639950ae_possession\n",
      "• stats_639950ae_misc\n",
      "• keeper_stats_639950ae\n",
      "• stats_6f7e1f03_summary\n",
      "• stats_6f7e1f03_passing\n",
      "• stats_6f7e1f03_passing_types\n",
      "• stats_6f7e1f03_defense\n",
      "• stats_6f7e1f03_possession\n",
      "• stats_6f7e1f03_misc\n",
      "• keeper_stats_6f7e1f03\n",
      "• shots_all\n",
      "• shots_639950ae\n",
      "• shots_6f7e1f03\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://fbref.com/en/matches/44e47b4d/\"\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "for table in soup.find_all(\"table\"):\n",
    "    print(\"•\", table.get(\"id\") or \"(no id)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
